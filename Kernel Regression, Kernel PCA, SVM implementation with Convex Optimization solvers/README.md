These assignments were done as part of a 1 Credit course "EE5605 - Kernel Methods" :

PART 1 :

3. Implement the vanilla (linear) and kernel versions of a support vector machine (SVM). Use python. Do not use built-in
functions for SVM. You can use open-source solvers for the convex optimization. (20)

4. Use Xsvm.csv, ysvm.csv to train the linear classifier. Test on the following points x1
t = [1.9, 0.4], x2
t = [0.9, 0.9], x3
t =
[1, 4, 1.5], x4
t = [0.01, 0.005]. Report your predictions. (10)

5. Demonstrate the ability of the kernel SVM to learn the non-linear XOR function. Experiment with different kernels.
Report the result of the kernel SVM and compare it with the vanilla version. (10)

PART 2 - 

3. Implement kernel ridge regression by kernelising the linear ridge regression predictor. Demonstrate the benefit of
kernelising this problem using noisy sinusoidal data points. (10)

4. Implement the vanilla (linear) and kernel versions of principal component analysis (PCA). To demonstrate the utility
of kernel PCA, generate data points for which the vanilla PCA fails while kernel PCA works. Clearly describe how
you generated your data points. (10)
